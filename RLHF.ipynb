{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:13:34.199775Z",
     "start_time": "2025-02-01T18:13:34.194512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, GenerationConfig\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, TaskType\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
    "from trl import create_reference_model\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ],
   "id": "819f6906c70e96dc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:13:47.668402Z",
     "start_time": "2025-02-01T18:13:34.207986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load dataset from hugging face dialogsum\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset_original = load_dataset(huggingface_dataset_name)\n",
    "dataset_original"
   ],
   "id": "434ea9a32a723eca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/4.65k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e1bc138abbf9435da57dcadd1201f780"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:14:04.489373Z",
     "start_time": "2025-02-01T18:13:47.680181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Take a subset of the dataset and filter only long enough and easy to read\n",
    "# Then wrap each dialog with the instruction and tokenize the prompt\n",
    "# Save the token_ids in the field token)ids\n",
    "# Decoded version of prompt in the field query\n",
    "\n",
    "def build_dataset(model_name, dataset_name, input_min_text_length, input_max_text_length):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset and split into training and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name: name of the model\n",
    "    - dataset_name: name of the dataset\n",
    "    - input_min_text_length: minimum text length\n",
    "    - input_max_text_length: maximum text length\n",
    "\n",
    "    Returns:\n",
    "    - dataset_splits (datasets.dataset_dict.DatasetDict): Processed dataset containing training and test sets.\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    dataset = dataset.filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, device=\"auto\")\n",
    "\n",
    "    def tokenize(sample):\n",
    "\n",
    "        # wrap each dialogue with the instruction\n",
    "        prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{sample[\"dialogue\"]}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "        sample[\"input_ids\"] = tokenizer.encode(prompt)\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=False)\n",
    "    dataset.set_format(type=\"torch\")\n",
    "    dataset_splits = dataset.train_test_split(test_size=0.2, shuffle=False, seed=42)\n",
    "    return dataset_splits\n",
    "\n",
    "dataset = build_dataset(model_name, huggingface_dataset_name, 200, 1000)\n",
    "print(dataset)"
   ],
   "id": "c340f89d28be2b64",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10022 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77626aec022242d0b15a59521ef86bb7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n",
      "        num_rows: 8017\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n",
      "        num_rows: 2005\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:14:04.498123Z",
     "start_time": "2025-02-01T18:14:04.495823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_number_of_trainable_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ],
   "id": "81b7ce56ca1a07e7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:14:10.670928Z",
     "start_time": "2025-02-01T18:14:04.518210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# use fine-tuned PEFT model with summarization instructions from previous project\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model,\n",
    "                                       \"intotheverse/peft-dialogue-summary-checkpoint\",\n",
    "                                       lora_config=lora_config,\n",
    "                                       torch_dtype=torch.float32,\n",
    "                                       device_map={\"\": device},\n",
    "                                       is_trainable=True)\n",
    "\n",
    "print(f'PEFT model parameters to be updated:\\n{print_number_of_trainable_parameters(peft_model)}')"
   ],
   "id": "babca00c887ae40e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/334 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f48378966e20491dbcdae41a50702248"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/14.2M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b54c4765b514b84836c6a41b78f620e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model parameters to be updated:\n",
      "\n",
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 1.41%\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:14:10.688006Z",
     "start_time": "2025-02-01T18:14:10.677542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare reward model\n",
    "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model,\n",
    "                                                               torch_dtype=torch.float32,\n",
    "                                                               is_trainable=True,\n",
    "                                                               device_map={\"\": device})\n",
    "ppo_model.generation_config = ppo_model.config\n",
    "# ppo_model.base_model_prefix = \"encoder\"\n",
    "# ppo_model.encoder = ppo_model.pretrained_model.encoder\n",
    "print(f'PPO model parameters to be updated (ValueHead + 769 parameters):\\n{print_number_of_trainable_parameters(ppo_model)}\\n')\n",
    "print(ppo_model.v_head)"
   ],
   "id": "a3d6438b9777ed6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO model parameters to be updated (ValueHead + 769 parameters):\n",
      "\n",
      "trainable model parameters: 3539713\n",
      "all model parameters: 251117569\n",
      "percentage of trainable model parameters: 1.41%\n",
      "\n",
      "ValueHead(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (summary): Linear(in_features=768, out_features=1, bias=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:14:11.442877Z",
     "start_time": "2025-02-01T18:14:10.701614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ref_model = create_reference_model(ppo_model).to(device)\n",
    "print(f'Reference model parameters to be upldated:\\n{print_number_of_trainable_parameters(ref_model)}')"
   ],
   "id": "b169e7c3591ee8b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference model parameters to be upldated:\n",
      "\n",
      "trainable model parameters: 0\n",
      "all model parameters: 251117569\n",
      "percentage of trainable model parameters: 0.00%\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:14:41.376465Z",
     "start_time": "2025-02-01T18:14:11.477814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a reference model\n",
    "toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "toxicity_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name, device_map={\"\": device})\n",
    "toxicity_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name, device_map={\"\": device})\n",
    "print(toxicity_model.config.id2label)"
   ],
   "id": "cf298608f5f276a0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41da982614704e53a50ae9b4f1fd7cda"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0632e087961410aa401e811b0253426"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51141369a4324bf6b02d21b619a81cb0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d7d7178a00e4776b8e36f97a2ed99bb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/816 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65d1e7fbe8ed40a390ef6f6e62e1c5e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a507b9da69d48919d15f742ab15de97"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'nothate', 1: 'hate'}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:14:41.861169Z",
     "start_time": "2025-02-01T18:14:41.419910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example of reward for a non-toxic text\n",
    "non_toxic_text = \"#Person 1# tells Tommy that he didn't like the movie.\"\n",
    "toxicity_input_ids = toxicity_tokenizer(non_toxic_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "logits = toxicity_model(input_ids=toxicity_input_ids).logits\n",
    "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "not_hate_index = 0\n",
    "not_hate_reward = (logits[:, not_hate_index].tolist())\n",
    "print(f'reward (high): {not_hate_reward}')"
   ],
   "id": "7a7aad56a8686cb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits [not hate, hate]: [3.114098310470581, -2.4896152019500732]\n",
      "probabilities [not hate, hate]: [0.9963293671607971, 0.003670633537694812]\n",
      "reward (high): [3.114098310470581]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:14:42.021995Z",
     "start_time": "2025-02-01T18:14:41.946479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example of reward for a toxic text\n",
    "toxic_text = \"#Person 1# tells Tommy that the movie was terrible, dumb and stupid.\"\n",
    "toxicity_input_ids = toxicity_tokenizer(toxic_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "logits = toxicity_model(input_ids=toxicity_input_ids).logits\n",
    "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "not_hate_reward = (logits[:, not_hate_index].tolist())\n",
    "print(f'reward (low): {not_hate_reward}')"
   ],
   "id": "3f69f1a2d1faf5b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits [not hate, hate]: [-0.692116916179657, 0.37227126955986023]\n",
      "probabilities [not hate, hate]: [0.25647175312042236, 0.7435283064842224]\n",
      "reward (low): [-0.692116916179657]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:14:42.882565Z",
     "start_time": "2025-02-01T18:14:42.035434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a hugging face pipeline to simplify the reward model\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\",\n",
    "                          model=toxicity_model_name,\n",
    "                          device=device)\n",
    "\n",
    "reward_logits_kwargs = {\n",
    "    \"top_k\": None, # Return all scores (not just the top prediction)\n",
    "    \"function_to_apply\": \"none\", # PPO uses raw logits for reward estimation\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "reward_probabilities_kwargs = {\n",
    "    \"top_k\": None,\n",
    "    \"function_to_apply\": \"softmax\", # Easier to interpret since it shows confidence scores\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "print(\"Reward model output:\")\n",
    "print(\"For non-toxic text\")\n",
    "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))\n",
    "print(\"For toxic text\")\n",
    "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"
   ],
   "id": "b7e11a69be840c53",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward model output:\n",
      "For non-toxic text\n",
      "[{'label': 'nothate', 'score': 3.114098310470581}, {'label': 'hate', 'score': -2.4896152019500732}]\n",
      "[{'label': 'nothate', 'score': 0.9963293671607971}, {'label': 'hate', 'score': 0.0036706337705254555}]\n",
      "For toxic text\n",
      "[{'label': 'hate', 'score': 0.37227126955986023}, {'label': 'nothate', 'score': -0.692116916179657}]\n",
      "[{'label': 'hate', 'score': 0.7435281872749329}, {'label': 'nothate', 'score': 0.25647175312042236}]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:14:45.786528Z",
     "start_time": "2025-02-01T18:14:42.913906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up an evaluation metric for toxicity\n",
    "toxicity_evaluator = evaluate.load(\"toxicity\",\n",
    "                                   toxicity_model_name,\n",
    "                                   module_type=\"measurement\",\n",
    "                                   toxic_label=\"hate\",\n",
    "                                   device=device)"
   ],
   "id": "5ac865789b8c0c1a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:14:45.880907Z",
     "start_time": "2025-02-01T18:14:45.818030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "toxicity_score = toxicity_evaluator.compute(predictions=[non_toxic_text, toxic_text])\n",
    "\n",
    "print(f\"Toxicity score for non-toxic text and toxic text: {toxicity_score[\"toxicity\"]}\")"
   ],
   "id": "9bc2fb0ae6918d09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity score for non-toxic text and toxic text: [0.0036706337705254555, 0.7435281872749329]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:14:45.911349Z",
     "start_time": "2025-02-01T18:14:45.908274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_toxicity(model,\n",
    "                      toxicity_evaluator,\n",
    "                      tokenizer,\n",
    "                      dataset,\n",
    "                      num_samples,\n",
    "                      device):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset and split it into training and test sets.\n",
    "\n",
    "    Parameters:\n",
    "        - model (trl model): model to be evaluated.\n",
    "        - toxicity_evaluator: toxicity evaluator.\n",
    "        - tokenizer (transformers tokenizer): tokenizer to be used.\n",
    "        - dataset (dataset): dataset to be evaluated.\n",
    "        - num_samples (int): number of samples to be evaluated.\n",
    "\n",
    "    Returns:\n",
    "        tuple: a tuple containing two numpy.float64 values:\n",
    "            - mean of samples toxicity\n",
    "            - standard deviation of samples toxicity\n",
    "    \"\"\"\n",
    "\n",
    "    toxicities = []\n",
    "    for i, sample in tqdm(enumerate(dataset)):\n",
    "        input_text = sample[\"query\"]\n",
    "        if i > num_samples:\n",
    "            break\n",
    "\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "        generation_config = GenerationConfig(max_new_tokens=100,\n",
    "                                             do_sample=True,\n",
    "                                             top_k=0.0,\n",
    "                                             top_p=1.0)\n",
    "        response_token_ids = model.generate(input_ids=input_ids, generation_config=generation_config)\n",
    "        generated_text_output = tokenizer.decode(response_token_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        toxicity_score = toxicity_evaluator.compute(predictions=[(input_text + \" \" + generated_text_output)])\n",
    "        toxicities.extend(toxicity_score[\"toxicity\"])\n",
    "\n",
    "    mean = np.mean(toxicities)\n",
    "    std = np.std(toxicities)\n",
    "    return mean, std"
   ],
   "id": "c12e2fc9efd5ea7d",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:15:16.406878Z",
     "start_time": "2025-02-01T18:14:45.934736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assess toxicity of PEFT trained model before RLHF\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map={\"\": device})\n",
    "mean_before_rlhf, std_before_rlhf = evaluate_toxicity(model=ref_model,\n",
    "                                                      toxicity_evaluator=toxicity_evaluator,\n",
    "                                                      tokenizer=tokenizer,\n",
    "                                                      dataset=dataset[\"test\"],\n",
    "                                                      num_samples=10,\n",
    "                                                      device=device)\n",
    "print(f\"Toxicity[mean, std] before RLHF: [{mean_before_rlhf}, {std_before_rlhf}]\")"
   ],
   "id": "cce792a9ae292d5e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:30,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity[mean, std] before RLHF: [0.03948024806397205, 0.047522372034200566]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:15:29.359699Z",
     "start_time": "2025-02-01T18:15:29.355953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a collator function, a data collector is used to batch process the training data before passing it to PPO\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "test_data = [\n",
    "    {\"query\": \"Summarize this text\", \"response\": \"Summary A\", \"reward\": 0.8},\n",
    "    {\"query\": \"Explain this topic\", \"response\": \"Explanation B\", \"reward\": 0.6}\n",
    "]\n",
    "print(collator(test_data))"
   ],
   "id": "c059148f8356815c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': ['Summarize this text', 'Explain this topic'], 'response': ['Summary A', 'Explanation B'], 'reward': [0.8, 0.6]}\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:15:44.607579Z",
     "start_time": "2025-02-01T18:15:43.797306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_rate=1.41e-5\n",
    "max_ppo_epochs=1\n",
    "mini_batch_size=4\n",
    "batch_size=16\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=learning_rate,\n",
    "    ppo_epochs=max_ppo_epochs,\n",
    "    mini_batch_size=mini_batch_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(config=config,\n",
    "                         model=ppo_model,\n",
    "                         ref_model=ref_model,\n",
    "                         tokenizer=tokenizer,\n",
    "                         dataset=dataset[\"train\"],\n",
    "                         data_collator=collator)"
   ],
   "id": "c69ce5924b00f0f7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/trl/trainer/ppo_config.py:207: FutureWarning: `PPOConfig` is deprecated and will be removed in the future. Please use `PPOv2Config` with `PPOv2Trainer` instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/trl/trainer/ppo_trainer.py:193: FutureWarning: `PPOTrainer` is deprecated and will be removed in trl v0.12. Please use `PPOv2Trainer` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-01T18:16:18.094318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_min_length = 100\n",
    "output_max_length = 400\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": 5,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True\n",
    "}\n",
    "\n",
    "reward_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"none\", # You want the raw logits without softmax.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "max_ppo_steps = 10\n",
    "\n",
    "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    # Break when you reach max_steps.\n",
    "    if step >= max_ppo_steps:\n",
    "        break\n",
    "\n",
    "    prompt_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    # Get response from FLAN-T5/PEFT LLM.\n",
    "    summary_tensors = []\n",
    "\n",
    "    for prompt_tensor in prompt_tensors:\n",
    "        max_new_tokens = output_length_sampler()\n",
    "\n",
    "        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
    "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
    "\n",
    "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
    "\n",
    "    # This needs to be called \"response\".\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
    "\n",
    "    # Compute reward outputs.\n",
    "    query_response_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n",
    "\n",
    "    # You use the `nothate` item because this is the score for the positive `nothate` class.\n",
    "    reward_tensors = [torch.tensor(reward[not_hate_index][\"score\"]) for reward in rewards]\n",
    "\n",
    "    # Run PPO step.\n",
    "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
    "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
    "\n",
    "    print(f'objective/kl: {stats[\"objective/kl\"]}')\n",
    "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
    "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n",
    "    print('-'.join('' for x in range(100)))"
   ],
   "id": "a3096ca918e09b5b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "1it [00:59, 59.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 30.84881591796875\n",
      "ppo/returns/mean: -0.8026288747787476\n",
      "ppo/policy/advantages_mean: -0.003186337649822235\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [02:11, 66.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 34.12253952026367\n",
      "ppo/returns/mean: -0.9107413291931152\n",
      "ppo/policy/advantages_mean: 0.0091189444065094\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [03:07, 61.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 26.020965576171875\n",
      "ppo/returns/mean: -0.6090663075447083\n",
      "ppo/policy/advantages_mean: 0.04889874905347824\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bff631e60befff33"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
